// src-tauri/src/backend/monitor.rs
// Continuous backend process monitoring

use std::process::Child;
use std::sync::{Arc, Mutex};
use std::time::Duration;

use super::config::BackendConfig;
use super::state::BackendState;

pub struct BackendMonitor {
    state: Arc<Mutex<BackendState>>,
    child: Arc<Mutex<Option<Child>>>,
    config: Arc<Mutex<Option<BackendConfig>>>,
}

impl BackendMonitor {
    pub fn new() -> Self {
        Self {
            state: Arc::new(Mutex::new(BackendState::NotStarted)),
            child: Arc::new(Mutex::new(None)),
            config: Arc::new(Mutex::new(None)),
        }
    }

    pub fn get_state(&self) -> BackendState {
        *self.state.lock().unwrap()
    }

    pub fn set_state(&self, state: BackendState) {
        *self.state.lock().unwrap() = state;
        log::info!("üìä Backend state changed: {}", state);
    }

    pub fn set_child(&self, child: Child) {
        *self.child.lock().unwrap() = Some(child);
        log::info!("üíæ Backend process stored");
    }

    pub fn set_config(&self, cfg: BackendConfig) {
        *self.config.lock().unwrap() = Some(cfg);
    }

    pub fn kill_child(&self) -> Result<(), String> {
        let mut child_lock = self.child.lock().unwrap();
        if let Some(mut child) = child_lock.take() {
            log::info!("üõë Terminating backend process...");
            child.kill().map_err(|e| format!("Failed to kill process: {}", e))?;
            log::info!("‚úÖ Backend process terminated");
            Ok(())
        } else {
            log::warn!("‚ö†Ô∏è No child process to terminate");
            Ok(())
        }
    }
}

// Global monitor instance
lazy_static::lazy_static! {
    static ref MONITOR: BackendMonitor = BackendMonitor::new();
}

pub fn monitor_backend(config: &BackendConfig, app: &tauri::AppHandle, child: Child) {
    log::info!("üëÅÔ∏è Starting backend monitoring...");

    MONITOR.set_state(BackendState::Starting);
    MONITOR.set_child(child); // Store child process for later termination
    MONITOR.set_config(config.clone());

    // Periodic health checks
    let app_handle = app.clone();
    let config_clone = config.clone();

    std::thread::spawn(move || {
        let mut consecutive_failures = 0;

        loop {
            std::thread::sleep(Duration::from_secs(config_clone.health_check_interval_secs));

            if MONITOR.get_state().is_stopped() {
                log::info!("‚èπÔ∏è Backend monitoring stopped (process not running)");
                break;
            }

            match super::health::wait_until_healthy_blocking(&config_clone) {
                Ok(_health) => {
                    if MONITOR.get_state() != BackendState::Healthy {
                        log::info!("‚úÖ Backend recovered to healthy state");
                        MONITOR.set_state(BackendState::Healthy);
                        crate::events::emit_backend_ready(&app_handle);
                    }
                    consecutive_failures = 0;
                }
                Err(_e) => {
                    consecutive_failures += 1;

                    if consecutive_failures >= 3 {
                        if MONITOR.get_state() != BackendState::Unhealthy {
                            log::warn!(
                                "‚ö†Ô∏è Backend is unhealthy ({} consecutive failures)",
                                consecutive_failures
                            );
                            MONITOR.set_state(BackendState::Unhealthy);
                            crate::events::emit_backend_unhealthy(&app_handle);
                        }
                    }
                }
            }
        }
    });
}

pub fn get_backend_state() -> BackendState {
    MONITOR.get_state()
}

pub fn kill_backend() -> Result<(), String> {
    MONITOR.kill_child()
}

/// Trigger a backup via API and then terminate backend
/// 
/// This function waits for the backup to complete OR for the shutdown timeout to expire
/// before killing the backend process. This ensures backups have enough time to complete.
pub fn trigger_backup_and_shutdown() -> Result<(), String> {
    // Try to trigger manual backup first (best-effort)
    if let Some(cfg) = MONITOR.config.lock().unwrap().clone() {
        let url = format!("{}/backups/trigger", cfg.backend_url());
        let timeout_secs = cfg.shutdown_timeout_secs;
        
        log::info!("üß© Triggering manual backup before shutdown: {}", url);
        log::info!("‚è±Ô∏è Waiting up to {} seconds for backup to complete", timeout_secs);
        
        let client = reqwest::blocking::Client::new();
        let res = client
            .post(url)
            .timeout(Duration::from_secs(timeout_secs))
            .send();
        
        match res {
            Ok(r) => {
                if r.status().is_success() {
                    log::info!("‚úÖ Manual backup completed successfully before shutdown");
                } else {
                    log::warn!("‚ö†Ô∏è Manual backup returned status {} - proceeding with shutdown", r.status());
                }
            }
            Err(e) => {
                // Check if it was a timeout error
                if e.is_timeout() {
                    log::warn!("‚ö†Ô∏è Manual backup timed out after {} seconds - proceeding with shutdown", timeout_secs);
                } else {
                    log::warn!("‚ö†Ô∏è Manual backup failed: {} - proceeding with shutdown", e);
                }
            }
        }
    } else {
        log::warn!("‚ö†Ô∏è No backend config available for manual backup trigger");
    }

    // Only kill the backend after backup completed or timeout expired
    log::info!("üõë Terminating backend process after backup attempt");
    kill_backend()
}
